---
  title: "Lab 3"
author: "Joseph Shamsian"
output: pdf_document
due date: "11:59PM February 24, 2019"
---
  
  ## Perceptron
  
  You will code the "perceptron learning algorithm". Take a look at the comments above the function. This is standard "Roxygen" format for documentation. Hopefully, we will get to packages at some point and we will go over this again. It is your job also to fill in this documentation.

```{r}
#' Perceptron Learning Algorithim: Provide a name for this function 
#' 
#':Explain what this function does in a few sentences
#' This function creates a line between serperable data, assuming the input is linear seperable. 
#' @param Xinput      matrix of size n X p. Where x is the number of observations and p is the number of features. 
#' @param y_binary    Output binary vector of values of length p +1 
#' @param MAX_ITER    Defaults to 1000:
#' @param w           Initial weight vector of length p +1 of type vector. If left unspecified the weight vector will be initialized to 0. 
#'
#' @return            The computed final parameter (weight) as a vector of length p + 1
#' @export            
#'
#' @author            [Joseph Shamsian]
perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 1000, w = NULL){
  
  p = ncol(Xinput)
  n = nrow(Xinput)
  
  if(is.null(w)){
    w = rep(0, (p + 1)) # zero vector w p+1 times
  }
  
  Xinput = cbind(1, Xinput)
  
  for (iter in 1 : MAX_ITER){  
    for (i in 1 : n){ 
      x_i = Xinput[i, ]
      yhat_i = ifelse(sum(x_i * w) > 0, 1, 0)
      y_i = y_binary[i]
      
      
      for(j in 1:(p +1)){
        
        w[j] = w[j] + (y_i - yhat_i) * x_i[j] #update for each feature
        
      }
    }
  }
  w
  
}
```

To understand what the algorithm is doing - linear "discrimination" between two response categories, we can draw a picture. First let's make up some very simple training data $\mathbb{D}$.

```{r}
Xy_simple = data.frame(
response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)

```

We are going to just get some plots and not talk about the code to generate them as we will have a whole unit on visualization using `ggplot2` in the future.

Let's first plot $y$ by the two features so the coordinate plane will be the two features and we use different colors to represent the third dimension, $y$.

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

TO-DO: Explain this picture.

This picture graphs the points in the data set. 

Now, let us run the algorithm and see what happens:
  
```{r}
w_vec_simple_per = perceptron_learning_algorithm(
  cbind(Xy_simple$first_feature, Xy_simple$second_feature),
  as.numeric(Xy_simple$response == 1))
w_vec_simple_per
```

TO-DO: Explain this output. What do the numbers mean? What is the intercept of this line and the slope? You will have to do some algebra.

-7,4 and -1 represent the w_vec for the perceptron algorithim. w_1 = -7, w_2 = 4, w_3 = -1. Intercept = 2.33 Slope = 4

```{r}
simple_perceptron_line = geom_abline(
  intercept = -w_vec_simple_per[1] / w_vec_simple_per[3], 
  slope = -w_vec_simple_per[2] / w_vec_simple_per[3], 
  color = "orange")
simple_viz_obj + simple_perceptron_line
```


TO-DO: Explain this picture. Why is this line of separation not "satisfying" to you?

The line is this picture is not satisfying to me because it does not produce the optimal wedge.

```{r}
rm(list=ls())
```

 

## Support Vector Machine
  
  
```{r}
X_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
y_binary = as.numeric(Xy_simple$response == 1)
```

Use the `e1071` package to fit an SVM model to `y_binary` using the features in `X_simple_feature_matrix`. Do not specify the $\lambda$ (i.e. do not specify the `cost` argument). Call the model object `svm_model`. Otherwise the remaining code won't work.

```{r}
#TO-DO copy from class
pacman::p_load(e1071)
svm_model =  svm(X_simple_feature_matrix, Xy_simple$response, kernel = "linear", scale = FALSE)
```

and then use the following code to visualize the line in purple:

```{r}
w_vec_simple_svm = c(
svm_model$rho, #the b term
-t(svm_model$coefs) %*% X_simple_feature_matrix[svm_model$index, ] # the other terms
)
simple_svm_line = geom_abline(
intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
color = "purple")
simple_viz_obj + simple_perceptron_line + simple_svm_line
```

Is this SVM line a better fit than the perceptron?

Yes. This line creates optimal wedge for future points. 
TO-DO

3. Now write pseuocode for your own implementation of the linear support vector machine algorithm respecting the following spec **making use of the nelder mead `optimx` function from lecture 5p**. It turns out you do not need to load the package `neldermead` to use this function. You can feel free to define a function within this function if you wish. 

Note there are differences between this spec and the perceptron learning algorithm spec in question \#1. You should figure out a way to respect the `MAX_ITER` argument value. 


```{r}
#' Support Vector Machine 
#
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
#TO-DO: write pseudo code in comments
#'
#'
#' h* = ifelse( (y_i-.5)*(W_Vec%*%Xinput + b>=-.5),1,0)
#'AHE = sum(h*)/h*.length()
#'sol = min(AHE + lambda*(w_Vec%*%W_VEC))
#'sol
#'
#'
#'
#'
#'
  
}
```


If you are enrolled in 390 the following is extra credit but if you're enrolled in 650, the following is required. Write the actual code. You may want to take a look at the `optimx` package we discussed in class.

```{r}
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
  #TO-DO EXTRA CREDIT
}
```

If you wrote code (the extra credit), run your function using the defaults and plot it in brown vis-a-vis the previous model's line:

```{r}
svm_model_weights = linear_svm_learning_algorithm(X_simple_feature_matrix, y_binary)
my_svm_line = geom_abline(
intercept = svm_model_weights[1] / svm_model_weights[3],#NOTE: negative sign removed from intercept argument here
slope = -svm_model_weights[2] / svm_model_weights[3], 
color = "brown")
simple_viz_obj  + my_svm_line
```
```{R}
rm(list=ls())
```
Is this the same as what the `e1071` implementation returned? Why or why not?


4. Write a $k=1$ nearest neighbor algorithm using the Euclidean distance function. Respect the spec below:


```{r}

#' This function implements the nearest neighbor algorithm.
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param Xtest       The test data that the algorithm will predict on as a n* x p matrix.
#' @return            The predictions as a n* length vector.
nn_algorithm_predict = function(Xinput, y_binary, Xtest){
  #TO-DO find the closest n(observation)
  dsqd = function(Xinput, Xtest){
    totalDist = 0
    for(i in 1:length(Xinput) ){ #go through all the features (p)of the nth observation
      totalDist = totalDist + sqrt((Xinput[i]-Xtest[i])^2) #find the distance between the vectors
    }
    totalDist
  }
  
  best_sqd_distance = dsqd(Xinput[1,], Xtest) #good place to begin
  i_star = y_binary[1]
  for (i in 2 : nrow(Xinput)){ # go through all the observation (n)
    if( dsqd(Xinput[i,], Xtest) ){
      best_sqd_distance = dsqd(Xinput[i,], Xtest)
      i_star = i
    }
  }
    
  y_binary[i_star]
}
```

Write a few tests to ensure it actually works:
  
```{r}
#TO-DO
pacman::p_load(class)
Xy = na.omit(MASS::biopsy) #The "breast cancer" data with all observations with missing values dropped
X = as.matrix(Xy[, 2 : 3]) #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")

y_hat = nn_algorithm_predict(X, y_binary, c(1, 2))#for one x
y_hat
```


We now add an argument `d` representing any legal distance function to the `nn_algorithm_predict` function. Update the implementation so it performs NN using that distance function. Set the default function to be the Euclidean distance in the original function. Also, alter the documentation in the appropriate places.

```{r}
#TO-DO
#' This function implements the nearest neighbor algorithm.
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param Xtest       The test data that the algorithm will predict on as a n* x p matrix.
#' @return            The predictions as a n* length vector.
dsqd = function(Xinput, Xtest){
    totalDist = 0
    for(i in 1:length(Xinput) ){ #go through all the features (p)of the nth observation
      totalDist = totalDist + sqrt((Xinput[i]-Xtest)^2) #find the distance between the vectors
    }
  }
nn_algorithm_predict = function(Xinput, y_binary, Xtest, d = dsqd){
  #TO-DO
  
  best_sqd_distance = dsqd(Xinput[1,], Xtest) #good place to begin
  i_star = y_binary[1]
  for (i in 2 : nrow(Xinput)){ # go through all the observation (n)
    if( d(Xinput[i,], Xtest) ){
      best_sqd_distance = d(Xinput[i,], Xtest)
      i_star = i
    }
  }
    
  y_binary[i_star]
}

```

For extra credit (unless you're a masters student), add an argument `k` to the `nn_algorithm_predict` function and update the implementation so it performs KNN. In the case of a tie, choose $\hat{y}$ randomly. Set the default `k` to be the square root of the size of $\mathcal{D}$ which is an empirical rule-of-thumb popularized by the "Pattern Classification" book by Duda, Hart and Stork (2007). Also, alter the documentation in the appropriate places.
                  
```{r}
#TO-DO --- extra credit for undergrads
```
                  
                  
                  