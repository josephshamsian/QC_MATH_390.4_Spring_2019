---
title: "Lab 10"
author: "Joseph Shamsian"
output: pdf_document
date: "11:59PM May 12, 2019"
---

First load the tree-building package:

```{r}
options(java.parameters = "-Xmx4000m")
# pacman::p_install_gh("kapelner/YARF", subdir = "YARF", ref = "dev")
pacman::p_load(YARF)
```

Let's take a look at the simulated sine curve data (i.e. the illustration I drew on the board last class)

```{r}
pacman::p_load(tidyverse)
n_train = 500
x_max = 10
x_train = runif(n_train, 0, x_max)
y_train = sin(x_train) + rnorm(n_train, 0, 0.3)
ggplot(data.frame(x = x_train, y = y_train), aes(x, y)) + geom_point(lwd=0.6) 
```

create a test set from the this data generating process with size 1000.

```{r}
n_test = 1000
x_test = runif(n_test, 0, x_max)
y_test = sin(x_test) + rnorm(n_test, 0, 0.3)
ggplot(data.frame(x = x_test, y = y_test), aes(x, y)) + geom_point(lwd=0.6) 
```


Fit a linear model to this dataset and test out of sample to get an idea of the generalization error.

```{r}
linear_mod = lm(y_train ~ x_train)
se_oos = sd(y_test - predict(linear_mod, data.frame(x_train = x_test)))
se_oos
```

Fit a tree to this dataset where nodesize is 25.

```{r}
tree_mod = YARFCART(data.frame(x = x_train), y_train, nodesize = 25)
```

How many nodes and how deep is this tree?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Create an image of this tree's nodes and split rules.

```{r}
illustrate_trees(tree_mod, max_depth = 4)
```

Test this tree model's performance out of sample to get an idea of the generalization error.

```{r}
se_oos = sd(y_test - predict(tree_mod, data.frame(x = x_test)))
se_oos
```

Fit a tree to this dataset where nodesize is 1 and test out of sample.

```{r}
tree_mod = YARFCART(data.frame(x = x_train), y_train, nodesize = 1)
get_tree_num_nodes_leaves_max_depths(tree_mod)
se_oos = sd(y_test - predict(tree_mod, data.frame(x = x_test)))
se_oos
```

Create M = 200 bootstrap samples of the data and save in a list.

```{r}
M = 200
bootstrap_x_train = list()
bootstrap_y_train = list()

for(i in 1:M){
  bootstrap_indices = sample(1 : n_train, replace = TRUE, size = n_train)
  bootstrap_x_train[[i]] = x_train[bootstrap_indices]
  bootstrap_y_train[[i]] = y_train[bootstrap_indices]
}
```

Create a bag of M trees model where nodesize = 5 (the regression default). Use the call of `YARFCART`.

```{r}
tree_mods = list()
for (k in 1 : M){
  tree_mods[[k]] = YARFCART(data.frame(x = bootstrap_x_train[[k]]), bootstrap_y_train[[k]], nodesize = 5, calculate_oob_error = FALSE)
}
```

Test this bagged model out of sample.

```{r}
y_test_hats = matrix(NA, nrow = n_test, ncol = M)

for(k in 1 : M){
  y_test_hats[, k] = predict(tree_mods[[k]], data.frame(x = x_test))
  
}
y_test_hats = rowMeans(y_test_hats)

se_oos = sd(y_test - y_test_hats)
se_oos
```

Using the bootstrapped samples, find the oob error. This is hard!

```{r}
#TO-DO
```

Fit a random forest model (RF) to the data. Report oob error.

```{r}
#TO-DO
```

Test the RF model out of sample. Is this error lower than the bagged model? Is the error similar to its oob error?

```{r}
#TO-DO
```

Load the `diamonds' dataset. Sample 1,000 rows for training and 1,000 rows for testing.

```{r}
#TO-DO
```

Build a linear model and test.

```{r}
#TO-DO
```

Build a bagged model and test. You can use `YARFBAG`.

```{r}
#TO-DO
```

Build a RF model and test. You can use `YARF`.


```{r}
#TO-DO
```

Explain why the gains are small from linear regression -> bagged trees -> random Forests

#TO-DO

Use `mlr` to build a RF model that is optimally tuned for the hyperparameter `mtry` and test out of sample.

```{r}
#TO-DO
```

Load the `nycflights13` data and join the weather table to airport in the four ways we learned about. 


```{r}
#TO-DO
```

Explain in English what each of these joins is doing.

#TO-DO