---
title: "Lab 9"
author: "Joseph Shamsian"
output: pdf_document
date: "11:59PM April 14, 2019"
---

# "data wrangling / munging / carpentry" with dplyr. 

First load `dplyr`, `tidyr`, `magrittr` and `lubridate` in one line.

```{r}
pacman::p_load(dplyr, tidyr, magrittr, lubridate)
library(lubridate)
```

Load the `storms` dataset from the `dplyr` package and investigate it using `str` and `summary` and `head`. Which two columns should be converted to type factor? Do so below using the `mutate` and the overwrite pipe operator `%<>%`. Verify.

```{r}
data("storms")
str(storms)
summary(storms)
head(storms)
storms %<>% 
  mutate(name = factor(name), status = factor(status))
str(storms)
```

Reorder the columns so name is first, status is second, category is third and the rest are the same. Verify.

```{r}
storms %<>% 
  select(name, status, category, everything())
storms
```

Sort the dataframe by year (most recent first) then category of the storm (most severe first). Verify.

```{r}
storms %<>% 
  arrange(desc(year), desc(category))
storms
```

Create a new feature `wind_speed_per_unit_pressure`.

```{r}
storms %<>%
  mutate(wind_speed_per_unit_pressure = wind / pressure)
storms
```

Create a new feature: `average_diameter` which averages the two diameters.

```{r}
storms %<>%
  mutate(average_diameter = (ts_diameter + hu_diameter) / 2)
storms
```


Calculate the distance from each storm observation to Miami in a new variable `distance_to_miami`.

```{r}
MIAMI_COORDS = c(25.7617, -80.1918)


compute_globe_distance = function(destination, origin){
  destination_radians = destination*pi/180
  origin_radians = origin*pi/180
  lat_change = destination_radians[1] - origin_radians[1]
  lon_change = destination_radians[2] - origin_radians[2]
  a = (sin(lat_change/2))^2 + cos(origin_radians[1]) * cos(destination_radians[1]) * (sin(long_change/2))^2
  return(RAD_EARTH * 2 * asin(sqrt(h)))
}

storms %<>%
  rowwise() %>%
  mutate(distance_to_miami = compute_globe_distance(MIAMI_COORDS, c(lat, long))) %>%
  select(lat, long, distance_to_miami, everything())
storms
```

At home: convert year, month, day, hour into the variable `timestamp` using the `lubridate` package.

```{r}
library(lubridate)
storms %<>%
  mutate(timestamp = ymd_h(paste0(year, "-", month, "-", day, " ", hour)))
storms
```

At home: using the `lubridate` package, create new variables `day_of_week` which is a factor with levels "Sunday", "Monday", ... "Saturday" and `week_of_year` which is integer 1, 2, ..., 52.

```{r}
storms %<>%
  mutate(day_of_week = wday(timestamp, label = TRUE, abbr = FALSE),
         week_of_year = week(timestamp))
storms
```

Create a new data frame `serious_storms` which are category 3 and above hurricanes.

```{r}
serious_storms = storms %>% 
  filter(category >= 3)
serious_storms
```

In `serious_storms`, merge the variables lat and long together into `lat_long` with values `lat / long` as a string.

```{r}
serious_storms %<>%
  unite(lat_long, lat, long, sep = " / ")
serious_storms
```

Back to the main dataframe `storms`, create a new feature `decile_windspeed` by binning wind speed into 10 bins.

```{r}
storms %<>% 
  mutate(decile_windspeed =  factor(ntile(wind, 10)))
  storms
```

Let's summarize some data. Find the strongest storm by wind speed per year.

```{r}
storms %>%
  group_by(year) %>%
  summarize(max_wind_speed = max(wind))
```

For each status, find the average category, wind speed, pressure and diameters (do not allow the average to be NA).

```{r}
storms %>%
  group_by(status) %>%
  summarize(avg_category = mean(as.numeric(as.character(category))), avg_wind_speed = mean(wind), avg_pressure = mean(pressure), avg_ts_diameter = mean(ts_diameter, na.rm = TRUE), avg_hu_diameter = mean(hu_diameter, na.rm = TRUE))
```

For each named storm, find its maximum category, wind speed, pressure and diameters (do not allow the max to be NA) and the number of readings (i.e. observations).

```{r}
storms %>%
  group_by(name) %>%
  summarize(max_category = max(category), max_wind_speed = max(wind), max_pressure = max(pressure), max_diameter = max(hu_diameter))
  
```

For each category, find its average wind speed, pressure and diameters (do not allow the max to be NA).

```{r}
storms %<>%
  group_by(category, average_wind_speed = wind / category)
storms
```


At home: for each named storm, find its duration in hours.

```{r}
storms %>%
  group_by(name) %>%
  mutate(duration = 6*n())
```

For each named storm, find the distance from its starting position to ending position in kilometers.

```{r}
storms %>%
  group_by(name) %>%
  arrange(desc(timestamp)) %>%
  summarize(distance_from_start = 1.61*compute_globe_distance( c(last(lat), last(long)) , c(first(lat), first(long)) ) )
```

Now we want to transition to building real design matrices for prediction. We want to predict the following: given the first three readings of a storm, can you predict its maximum wind speed? Identify the `y` and identify which features you need $x_1, ... x_p$ and build that matrix with `dplyr` functions. This is not easy, but it is what it's all about. Feel free to "featurize" (as Dana Chandler spoke about) as creatively as you would like. You aren't going to overfit if you only build a few features relative to the total 198 storms.

```{r}
#TO-DO
```

# Interactions in linear models

Load the Boston Housing Data from package `MASS` and use `str` and `summary` to remind yourself of the features and their types and then use `?MASS::Boston` to read an English description of the features.

```{r}
data(Boston, package = "MASS")
str(Boston)
summary(Boston)
?MASS::Boston
```

Using your knowledge of the modeling problem, try to guess which features are interacting. Confirm using plots in `ggplot` that illustrate three (or more) features.

```{r}
pacman::p_load(ggplot2)
base = ggplot(Boston, aes(x = rm, y = medv))
base + geom_point(aes(col = crim))
```

Once an interaction has been located, confirm the "non-linear linear" model with the interaction term does better than just the vanilla linear model.

```{r}
mod = lm(medv ~ rm * crim, Boston)
coef(mod) 

mod_vanilla = lm(medv ~ rm + crim, Boston)
summary(mod_vanilla)$r.squared
summary(mod_vanilla)$sigma
summary(mod)$r.squared
summary(mod)$sigma

```

Repeat this procedure for another interaction with two different features (not used in the previous interaction you found) and verify.

```{r}
base + geom_point(aes(col = zn))
mod = lm(medv ~ rm * zn, Boston)
coef(mod) 

mod_vanilla = lm(medv ~ rm + zn, Boston)
summary(mod_vanilla)$r.squared
summary(mod_vanilla)$sigma
summary(mod)$r.squared
summary(mod)$sigma
```

Fit a model using all possible first-order interactions. Verify it is "better" than the linear model. Do you think you overfit? Why or why not?

```{r}
base + geom_point(aes(col = zn))
mod = lm(medv ~  (.)^2 , Boston)
coef(mod) 

mod_vanilla = lm(medv ~ rm + zn, Boston)
summary(mod_vanilla)$r.squared
summary(mod_vanilla)$sigma
summary(mod)$r.squared
summary(mod)$sigma

#no we did not overfit. The number of rows in the Boston housing data is 506 and we only used 91 features. 
```

# CV

Use 5-fold CV to estimate the generalization error of the model with all interactions.

```{r}
pacman::p_load(mlr)
library(mlr)
modeling_task = makeRegrTask(data = Boston, target = "medv") #instantiate the task
algorithm = makeLearner("regr.lm") #instantiate the OLS learner algorithm on the diamonds dataset and set y = price
validation = makeResampleDesc("CV", iters = 5) #instantiate the 5-fold CV
resample(algorithm, modeling_task, validation)
```